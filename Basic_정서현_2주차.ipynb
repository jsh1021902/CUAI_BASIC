{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSbSwpjed8/Xbb8xIPjq+9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsh1021902/CUAI_BASIC/blob/main/Basic_%EC%A0%95%EC%84%9C%ED%98%84_2%EC%A3%BC%EC%B0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 사이킷런 소개와 특징"
      ],
      "metadata": {
        "id": "7OTF_hb4RCNB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0eKLruUQXt9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 첫 번째 머신러닝 만들어 보기 - 붓꽃 품종 예측하기"
      ],
      "metadata": {
        "id": "tZ53USXQRJ2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "peIYLrjERTQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 붓꽃 데이터 세트를 로딩합니다. \n",
        "iris = load_iris()\n",
        "\n",
        "# iris.data는 Iris 데이터 세트에서 피처(feature)만으로 된 데이터를 numpy로 가지고 있습니다. \n",
        "iris_data = iris.data\n",
        "\n",
        "# iris.target은 붓꽃 데이터 세트에서 레이블(결정 값) 데이터를 numpy로 가지고 있습니다. \n",
        "iris_label = iris.target\n",
        "print('iris target값:', iris_label)\n",
        "print('iris target명:', iris.target_names)\n",
        "\n",
        "# 붓꽃 데이터 세트를 자세히 보기 위해 DataFrame으로 변환합니다. \n",
        "iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)\n",
        "iris_df['label'] = iris.target\n",
        "iris_df.head(3)"
      ],
      "metadata": {
        "id": "nPAfORRJTQhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, \n",
        "                                                    test_size=0.2, random_state=11)"
      ],
      "metadata": {
        "id": "MMNi5puvTS_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DecisionTreeClassifier 객체 생성 \n",
        "dt_clf = DecisionTreeClassifier(random_state=11)\n",
        "\n",
        "# 학습 수행 \n",
        "dt_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "XSgrQruLTaNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습이 완료된 DecisionTreeClassifier 객체에서 테스트 데이터 세트로 예측 수행. \n",
        "pred = dt_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "r50yCTJ1Tbol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))"
      ],
      "metadata": {
        "id": "5x-UPydTTcjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 사이킷런의 기반 프레임워크 익히기"
      ],
      "metadata": {
        "id": "nrEmxubTRJty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimator 이해 및 fit(), predict() 메서드"
      ],
      "metadata": {
        "id": "WDb9HZ3HRZpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris_data = load_iris()\n",
        "print(type(iris_data))"
      ],
      "metadata": {
        "id": "M4mfXJynRW0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = iris_data.keys()\n",
        "print('붓꽃 데이터 세트의 키들:', keys)"
      ],
      "metadata": {
        "id": "ScM15hsoTiVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n feature_names 의 type:',type(iris_data.feature_names))\n",
        "print(' feature_names 의 shape:',len(iris_data.feature_names))\n",
        "print(iris_data.feature_names)\n",
        "\n",
        "print('\\n target_names 의 type:',type(iris_data.target_names))\n",
        "print(' feature_names 의 shape:',len(iris_data.target_names))\n",
        "print(iris_data.target_names)\n",
        "\n",
        "print('\\n data 의 type:',type(iris_data.data))\n",
        "print(' data 의 shape:',iris_data.data.shape)\n",
        "print(iris_data['data'])\n",
        "\n",
        "print('\\n target 의 type:',type(iris_data.target))\n",
        "print(' target 의 shape:',iris_data.target.shape)\n",
        "print(iris_data.target)"
      ],
      "metadata": {
        "id": "0Ga_44gHTmAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 사이킷런의 주요 모듈"
      ],
      "metadata": {
        "id": "jes7kf6SRh6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R23w2OnGRpTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 내장된 예제 데이터 세트"
      ],
      "metadata": {
        "id": "tNr4AO19RpsP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o3hyiTpkRlyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model Selection 모듈 소개"
      ],
      "metadata": {
        "id": "Yr6TD2p2RXRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습/테스트 데이터 세트 분리 - train_test_split()"
      ],
      "metadata": {
        "id": "4zVFZieCR1Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "train_data = iris.data\n",
        "train_label = iris.target\n",
        "dt_clf.fit(train_data, train_label)\n",
        "\n",
        "# 학습 데이터 셋으로 예측 수행\n",
        "pred = dt_clf.predict(train_data)\n",
        "print('예측 정확도:',accuracy_score(train_label,pred))"
      ],
      "metadata": {
        "id": "nHOCghq3Rz7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dt_clf = DecisionTreeClassifier( )\n",
        "iris_data = load_iris()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, \n",
        "                                                    test_size=0.3, random_state=121)"
      ],
      "metadata": {
        "id": "2ShMXPF_TtvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_clf.fit(X_train, y_train)\n",
        "pred = dt_clf.predict(X_test)\n",
        "print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))"
      ],
      "metadata": {
        "id": "82sTEznUTu4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 교차 검증"
      ],
      "metadata": {
        "id": "m1uiEi-_R8F3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K 폴드"
      ],
      "metadata": {
        "id": "l6gixJBpTv79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "features = iris.data\n",
        "label = iris.target\n",
        "dt_clf = DecisionTreeClassifier(random_state=156)\n",
        "\n",
        "# 5개의 폴드 세트로 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성.\n",
        "kfold = KFold(n_splits=5)\n",
        "cv_accuracy = []\n",
        "print('붓꽃 데이터 세트 크기:',features.shape[0])"
      ],
      "metadata": {
        "id": "0xNwAsRrR_bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_iter = 0\n",
        "\n",
        "# KFold객체의 split( ) 호출하면 폴드 별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환  \n",
        "for train_index, test_index  in kfold.split(features):\n",
        "    # kfold.split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출\n",
        "    X_train, X_test = features[train_index], features[test_index]\n",
        "    y_train, y_test = label[train_index], label[test_index]\n",
        "    #학습 및 예측 \n",
        "    dt_clf.fit(X_train , y_train)    \n",
        "    pred = dt_clf.predict(X_test)\n",
        "    n_iter += 1\n",
        "    # 반복 시 마다 정확도 측정 \n",
        "    accuracy = np.round(accuracy_score(y_test,pred), 4)\n",
        "    train_size = X_train.shape[0]\n",
        "    test_size = X_test.shape[0]\n",
        "    print('\\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'\n",
        "          .format(n_iter, accuracy, train_size, test_size))\n",
        "    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))\n",
        "    cv_accuracy.append(accuracy)\n",
        "    \n",
        "# 개별 iteration별 정확도를 합하여 평균 정확도 계산 \n",
        "print('\\n## 평균 검증 정확도:', np.mean(cv_accuracy)) "
      ],
      "metadata": {
        "id": "I9X3sifgT2Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stratified K 폴드"
      ],
      "metadata": {
        "id": "f6Nh5oOCT3GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "iris_df['label']=iris.target\n",
        "iris_df['label'].value_counts()"
      ],
      "metadata": {
        "id": "JuFqRgKOT9PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=3)\n",
        "# kfold.split(X)는 폴드 세트를 3번 반복할 때마다 달라지는 학습/테스트 용 데이터 로우 인덱스 번호 반환. \n",
        "n_iter =0\n",
        "for train_index, test_index  in kfold.split(iris_df):\n",
        "    n_iter += 1\n",
        "    label_train= iris_df['label'].iloc[train_index]\n",
        "    label_test= iris_df['label'].iloc[test_index]\n",
        "    print('## 교차 검증: {0}'.format(n_iter))\n",
        "    print('학습 레이블 데이터 분포:\\n', label_train.value_counts())\n",
        "    print('검증 레이블 데이터 분포:\\n', label_test.value_counts())"
      ],
      "metadata": {
        "id": "9uqoLMpFT_Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits=3)\n",
        "n_iter=0\n",
        "\n",
        "for train_index, test_index in skf.split(iris_df, iris_df['label']):\n",
        "    n_iter += 1\n",
        "    label_train= iris_df['label'].iloc[train_index]\n",
        "    label_test= iris_df['label'].iloc[test_index]\n",
        "    print('## 교차 검증: {0}'.format(n_iter))\n",
        "    print('학습 레이블 데이터 분포:\\n', label_train.value_counts())\n",
        "    print('검증 레이블 데이터 분포:\\n', label_test.value_counts())"
      ],
      "metadata": {
        "id": "k-8DOjyUUA_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_clf = DecisionTreeClassifier(random_state=156)\n",
        "\n",
        "skfold = StratifiedKFold(n_splits=3)\n",
        "n_iter=0\n",
        "cv_accuracy=[]\n",
        "\n",
        "# StratifiedKFold의 split( ) 호출시 반드시 레이블 데이터 셋도 추가 입력 필요  \n",
        "for train_index, test_index  in skfold.split(features, label):\n",
        "    # split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출\n",
        "    X_train, X_test = features[train_index], features[test_index]\n",
        "    y_train, y_test = label[train_index], label[test_index]\n",
        "    #학습 및 예측 \n",
        "    dt_clf.fit(X_train , y_train)    \n",
        "    pred = dt_clf.predict(X_test)\n",
        "\n",
        "    # 반복 시 마다 정확도 측정 \n",
        "    n_iter += 1\n",
        "    accuracy = np.round(accuracy_score(y_test,pred), 4)\n",
        "    train_size = X_train.shape[0]\n",
        "    test_size = X_test.shape[0]\n",
        "    print('\\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'\n",
        "          .format(n_iter, accuracy, train_size, test_size))\n",
        "    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))\n",
        "    cv_accuracy.append(accuracy)\n",
        "    \n",
        "# 교차 검증별 정확도 및 평균 정확도 계산 \n",
        "print('\\n## 교차 검증별 정확도:', np.round(cv_accuracy, 4))\n",
        "print('## 평균 검증 정확도:', np.round(np.mean(cv_accuracy), 4))"
      ],
      "metadata": {
        "id": "xZopEwa6UCo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cross_val_score()"
      ],
      "metadata": {
        "id": "UgW6s17JUGHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score , cross_validate\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris_data = load_iris()\n",
        "dt_clf = DecisionTreeClassifier(random_state=156)\n",
        "\n",
        "data = iris_data.data\n",
        "label = iris_data.target\n",
        "\n",
        "# 성능 지표는 정확도(accuracy) , 교차 검증 세트는 3개 \n",
        "scores = cross_val_score(dt_clf , data , label , scoring='accuracy',cv=3)\n",
        "print('교차 검증별 정확도:',np.round(scores, 4))\n",
        "print('평균 검증 정확도:', np.round(np.mean(scores), 4))"
      ],
      "metadata": {
        "id": "tVcpHyg3UM9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GridSearchCV - 교차 검증과 최적 하이퍼 파라미터 튜닝을 한 번에"
      ],
      "metadata": {
        "id": "yCrysDVLSFDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 데이터를 로딩하고 학습데이타와 테스트 데이터 분리\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, \n",
        "                                                    test_size=0.2, random_state=121)\n",
        "dtree = DecisionTreeClassifier()\n",
        "\n",
        "### parameter 들을 dictionary 형태로 설정\n",
        "parameters = {'max_depth':[1,2,3], 'min_samples_split':[2,3]}"
      ],
      "metadata": {
        "id": "BKQ8q42iSMtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# param_grid의 하이퍼 파라미터들을 3개의 train, test set fold 로 나누어서 테스트 수행 설정.  \n",
        "### refit=True 가 default 임. True이면 가장 좋은 파라미터 설정으로 재 학습 시킴.  \n",
        "grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)\n",
        "\n",
        "# 붓꽃 Train 데이터로 param_grid의 하이퍼 파라미터들을 순차적으로 학습/평가 .\n",
        "grid_dtree.fit(X_train, y_train)\n",
        "\n",
        "# GridSearchCV 결과 추출하여 DataFrame으로 변환\n",
        "scores_df = pd.DataFrame(grid_dtree.cv_results_)\n",
        "scores_df[['params', 'mean_test_score', 'rank_test_score', \\\n",
        "           'split0_test_score', 'split1_test_score', 'split2_test_score']]"
      ],
      "metadata": {
        "id": "4bTMTDiHUZk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('GridSearchCV 최적 파라미터:', grid_dtree.best_params_)\n",
        "print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dtree.best_score_))"
      ],
      "metadata": {
        "id": "u6qVl5DfUbmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GridSearchCV의 refit으로 이미 학습이 된 estimator 반환\n",
        "estimator = grid_dtree.best_estimator_\n",
        "\n",
        "# GridSearchCV의 best_estimator_는 이미 최적 하이퍼 파라미터로 학습이 됨\n",
        "pred = estimator.predict(X_test)\n",
        "print('테스트 데이터 세트 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))"
      ],
      "metadata": {
        "id": "mfQNpAlKUclX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 데이터 전처리"
      ],
      "metadata": {
        "id": "2dYBnUdtSOsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 인코딩"
      ],
      "metadata": {
        "id": "DH_zWE36SRMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 레이블 인코딩 (Label encoding)"
      ],
      "metadata": {
        "id": "bMnZqzmeUhJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "items=['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
        "\n",
        "# LabelEncoder를 객체로 생성한 후 , fit( ) 과 transform( ) 으로 label 인코딩 수행. \n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(items)\n",
        "labels = encoder.transform(items)\n",
        "print('인코딩 변환값:',labels)"
      ],
      "metadata": {
        "id": "ysMdccOWSQSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('인코딩 클래스:',encoder.classes_)"
      ],
      "metadata": {
        "id": "auXW3QCEUqBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('디코딩 원본 값:',encoder.inverse_transform([4, 5, 2, 0, 1, 1, 3, 3]))"
      ],
      "metadata": {
        "id": "OvBZHOLVUrUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 원-핫 인코딩(One-Hot encoding)"
      ],
      "metadata": {
        "id": "j9WNU4E-UtkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "items=['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
        "\n",
        "# 2차원 ndarray로 변환합니다. \n",
        "items = np.array(items).reshape(-1, 1)\n",
        "\n",
        "# 원-핫 인코딩을 적용합니다. \n",
        "oh_encoder = OneHotEncoder()\n",
        "oh_encoder.fit(items)\n",
        "oh_labels = oh_encoder.transform(items)\n",
        "\n",
        "# OneHotEncoder로 변환한 결과는 희소행렬이므로 toarray()를 이용해 밀집 행렬로 변환. \n",
        "print('원-핫 인코딩 데이터')\n",
        "print(oh_labels.toarray())\n",
        "print('원-핫 인코딩 데이터 차원')\n",
        "print(oh_labels.shape)"
      ],
      "metadata": {
        "id": "yvaEcKedUuwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'item':['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서'] })\n",
        "pd.get_dummies(df)"
      ],
      "metadata": {
        "id": "bi_QhP6_UwWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 피처 스케일링과 정규화"
      ],
      "metadata": {
        "id": "UNz5tSJmSVZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## StandardScaler"
      ],
      "metadata": {
        "id": "A_6Sb6vuSZhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "# 붓꽃 데이터 셋을 로딩하고 DataFrame으로 변환합니다. \n",
        "iris = load_iris()\n",
        "iris_data = iris.data\n",
        "iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)\n",
        "\n",
        "print('feature 들의 평균 값')\n",
        "print(iris_df.mean())\n",
        "print('\\nfeature 들의 분산 값')\n",
        "print(iris_df.var())"
      ],
      "metadata": {
        "id": "3PMO74BESXfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# StandardScaler객체 생성\n",
        "scaler = StandardScaler()\n",
        "# StandardScaler 로 데이터 셋 변환. fit( ) 과 transform( ) 호출.  \n",
        "scaler.fit(iris_df)\n",
        "iris_scaled = scaler.transform(iris_df)\n",
        "\n",
        "#transform( )시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
        "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
        "print('feature 들의 평균 값')\n",
        "print(iris_df_scaled.mean())\n",
        "print('\\nfeature 들의 분산 값')\n",
        "print(iris_df_scaled.var())"
      ],
      "metadata": {
        "id": "OID0NLUbSc4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MinMaxScaler"
      ],
      "metadata": {
        "id": "b1xPF5JqSee0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# MinMaxScaler객체 생성\n",
        "scaler = MinMaxScaler()\n",
        "# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.  \n",
        "scaler.fit(iris_df)\n",
        "iris_scaled = scaler.transform(iris_df)\n",
        "\n",
        "# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
        "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
        "print('feature들의 최솟값')\n",
        "print(iris_df_scaled.min())\n",
        "print('\\nfeature들의 최댓값')\n",
        "print(iris_df_scaled.max())"
      ],
      "metadata": {
        "id": "B0_ngjF5Sgq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 데이터와 테스트 데이터의 스케일링 변환 시 유의점"
      ],
      "metadata": {
        "id": "N8a17BZiSiDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# 학습 데이터는 0 부터 10까지, 테스트 데이터는 0 부터 5까지 값을 가지는 데이터 세트로 생성\n",
        "# Scaler클래스의 fit(), transform()은 2차원 이상 데이터만 가능하므로 reshape(-1, 1)로 차원 변경\n",
        "train_array = np.arange(0, 11).reshape(-1, 1)\n",
        "test_array =  np.arange(0, 6).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "yQ4gDal_Sms4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MinMaxScaler 객체에 별도의 feature_range 파라미터 값을 지정하지 않으면 0~1 값으로 변환\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# fit()하게 되면 train_array 데이터의 최솟값이 0, 최댓값이 10으로 설정.\n",
        "scaler.fit(train_array)\n",
        "\n",
        "# 1/10 scale로 train_array 데이터 변환함. 원본 10-> 1로 변환됨.\n",
        "train_scaled = scaler.transform(train_array)\n",
        "\n",
        "print('원본 train_array 데이터:', np.round(train_array.reshape(-1), 2))\n",
        "print('Scale된 train_array 데이터:', np.round(train_scaled.reshape(-1), 2))"
      ],
      "metadata": {
        "id": "x5NBHNuFVJxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MinMaxScaler에 test_array를 fit()하게 되면 원본 데이터의 최솟값이 0, 최댓값이 5로 설정됨\n",
        "scaler.fit(test_array)\n",
        "\n",
        "# 1/5 scale로 test_array 데이터 변환함. 원본 5->1로 변환.\n",
        "test_scaled = scaler.transform(test_array)\n",
        "\n",
        "# test_array의 scale 변환 출력.\n",
        "print('원본 test_array 데이터:', np.round(test_array.reshape(-1), 2))\n",
        "print('Scale된 test_array 데이터:', np.round(test_scaled.reshape(-1), 2))"
      ],
      "metadata": {
        "id": "5bCfY9qmVNgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train_array)\n",
        "train_scaled = scaler.transform(train_array)\n",
        "print('원본 train_array 데이터:', np.round(train_array.reshape(-1), 2))\n",
        "print('Scale된 train_array 데이터:', np.round(train_scaled.reshape(-1), 2))\n",
        "\n",
        "# test_array에 Scale 변환을 할 때는 반드시 fit()을 호출하지 않고 transform() 만으로 변환해야 함. \n",
        "test_scaled = scaler.transform(test_array)\n",
        "print('\\n원본 test_array 데이터:', np.round(test_array.reshape(-1), 2))\n",
        "print('Scale된 test_array 데이터:', np.round(test_scaled.reshape(-1), 2))"
      ],
      "metadata": {
        "id": "oSvItryKVPCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. 사이킷런으로 수행하는 타이타닉 생존자 예측"
      ],
      "metadata": {
        "id": "Br7DtrGsSoX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "titanic_df = pd.read_csv('./titanic_train.csv')\n",
        "titanic_df.head(3)"
      ],
      "metadata": {
        "id": "R2SIO_sNSrmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n ### train 데이터 정보 ###  \\n')\n",
        "print(titanic_df.info())"
      ],
      "metadata": {
        "id": "rddgsxN8VSuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df['Age'].fillna(titanic_df['Age'].mean(),inplace=True)\n",
        "titanic_df['Cabin'].fillna('N',inplace=True)\n",
        "titanic_df['Embarked'].fillna('N',inplace=True)\n",
        "print('데이터 세트 Null 값 갯수 ',titanic_df.isnull().sum().sum())"
      ],
      "metadata": {
        "id": "yoBreGi3VUDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' Sex 값 분포 :\\n',titanic_df['Sex'].value_counts())\n",
        "print('\\n Cabin 값 분포 :\\n',titanic_df['Cabin'].value_counts())\n",
        "print('\\n Embarked 값 분포 :\\n',titanic_df['Embarked'].value_counts())"
      ],
      "metadata": {
        "id": "KYvugBkjVV4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df['Cabin'] = titanic_df['Cabin'].str[:1]\n",
        "print(titanic_df['Cabin'].head(3))"
      ],
      "metadata": {
        "id": "_ButNnKyVW6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df.groupby(['Sex','Survived'])['Survived'].count()"
      ],
      "metadata": {
        "id": "9icoGxx5VX1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x='Sex', y = 'Survived', data=titanic_df)"
      ],
      "metadata": {
        "id": "MmJSk8HYVhpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x='Pclass', y='Survived', hue='Sex', data=titanic_df)"
      ],
      "metadata": {
        "id": "2jAaEu9zViJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 age에 따라 구분값을 반환하는 함수 설정. DataFrame의 apply lambda식에 사용. \n",
        "def get_category(age):\n",
        "    cat = ''\n",
        "    if age <= -1: cat = 'Unknown'\n",
        "    elif age <= 5: cat = 'Baby'\n",
        "    elif age <= 12: cat = 'Child'\n",
        "    elif age <= 18: cat = 'Teenager'\n",
        "    elif age <= 25: cat = 'Student'\n",
        "    elif age <= 35: cat = 'Young Adult'\n",
        "    elif age <= 60: cat = 'Adult'\n",
        "    else : cat = 'Elderly'\n",
        "    \n",
        "    return cat\n",
        "\n",
        "# 막대그래프의 크기 figure를 더 크게 설정 \n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "#X축의 값을 순차적으로 표시하기 위한 설정 \n",
        "group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Elderly']\n",
        "\n",
        "# lambda 식에 위에서 생성한 get_category( ) 함수를 반환값으로 지정. \n",
        "# get_category(X)는 입력값으로 'Age' 컬럼값을 받아서 해당하는 cat 반환\n",
        "titanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x : get_category(x))\n",
        "sns.barplot(x='Age_cat', y = 'Survived', hue='Sex', data=titanic_df, order=group_names)\n",
        "titanic_df.drop('Age_cat', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "I0wHDqAUVjW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "def encode_features(dataDF):\n",
        "    features = ['Cabin', 'Sex', 'Embarked']\n",
        "    for feature in features:\n",
        "        le = preprocessing.LabelEncoder()\n",
        "        le = le.fit(dataDF[feature])\n",
        "        dataDF[feature] = le.transform(dataDF[feature])\n",
        "        \n",
        "    return dataDF\n",
        "\n",
        "titanic_df = encode_features(titanic_df)\n",
        "titanic_df.head()"
      ],
      "metadata": {
        "id": "Kz8TIH_LVlFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Null 처리 함수\n",
        "def fillna(df):\n",
        "    df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
        "    df['Cabin'].fillna('N', inplace=True)\n",
        "    df['Embarked'].fillna('N', inplace=True)\n",
        "    df['Fare'].fillna(0, inplace=True)\n",
        "    return df\n",
        "\n",
        "# 머신러닝 알고리즘에 불필요한 피처 제거\n",
        "def drop_features(df):\n",
        "    df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "# 레이블 인코딩 수행.\n",
        "def format_features(df):\n",
        "    df['Cabin'] = df['Cabin'].str[:1]\n",
        "    features = ['Cabin', 'Sex', 'Embarked']\n",
        "    for feature in features:\n",
        "        le = LabelEncoder()\n",
        "        le = le.fit(df[feature])\n",
        "        df[feature] = le.transform(df[feature])\n",
        "    return df\n",
        "\n",
        "# 앞에서 설정한 데이터 전처리 함수 호출\n",
        "def transform_features(df):\n",
        "    df = fillna(df)\n",
        "    df = drop_features(df)\n",
        "    df = format_features(df)\n",
        "    return df"
      ],
      "metadata": {
        "id": "TAtBf6oUVmck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 원본 데이터를 재로딩 하고, feature데이터 셋과 Label 데이터 셋 추출. \n",
        "titanic_df = pd.read_csv('./titanic_train.csv')\n",
        "y_titanic_df = titanic_df['Survived']\n",
        "X_titanic_df= titanic_df.drop('Survived',axis=1)\n",
        "\n",
        "X_titanic_df = transform_features(X_titanic_df)"
      ],
      "metadata": {
        "id": "yISeEBfiVqPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df, \\\n",
        "                                                  test_size=0.2, random_state=11)"
      ],
      "metadata": {
        "id": "Ms7HUBD1VrYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 결정트리, Random Forest, 로지스틱 회귀를 위한 사이킷런 Classifier 클래스 생성\n",
        "dt_clf = DecisionTreeClassifier(random_state=11)\n",
        "rf_clf = RandomForestClassifier(random_state=11)\n",
        "lr_clf = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# DecisionTreeClassifier 학습/예측/평가\n",
        "dt_clf.fit(X_train , y_train)\n",
        "dt_pred = dt_clf.predict(X_test)\n",
        "print('DecisionTreeClassifier 정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))\n",
        "\n",
        "# RandomForestClassifier 학습/예측/평가\n",
        "rf_clf.fit(X_train , y_train)\n",
        "rf_pred = rf_clf.predict(X_test)\n",
        "print('RandomForestClassifier 정확도:{0:.4f}'.format(accuracy_score(y_test, rf_pred)))\n",
        "\n",
        "# LogisticRegression 학습/예측/평가\n",
        "lr_clf.fit(X_train , y_train)\n",
        "lr_pred = lr_clf.predict(X_test)\n",
        "print('LogisticRegression 정확도: {0:.4f}'.format(accuracy_score(y_test, lr_pred)))"
      ],
      "metadata": {
        "id": "YjjPFvgdVsi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def exec_kfold(clf, folds=5):\n",
        "    # 폴드 세트를 5개인 KFold객체를 생성, 폴드 수만큼 예측결과 저장을 위한  리스트 객체 생성.\n",
        "    kfold = KFold(n_splits=folds)\n",
        "    scores = []\n",
        "    \n",
        "    # KFold 교차 검증 수행. \n",
        "    for iter_count , (train_index, test_index) in enumerate(kfold.split(X_titanic_df)):\n",
        "        # X_titanic_df 데이터에서 교차 검증별로 학습과 검증 데이터를 가리키는 index 생성\n",
        "        X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]\n",
        "        y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]\n",
        "        \n",
        "        # Classifier 학습, 예측, 정확도 계산 \n",
        "        clf.fit(X_train, y_train) \n",
        "        predictions = clf.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, predictions)\n",
        "        scores.append(accuracy)\n",
        "        print(\"교차 검증 {0} 정확도: {1:.4f}\".format(iter_count, accuracy))     \n",
        "    \n",
        "    # 5개 fold에서의 평균 정확도 계산. \n",
        "    mean_score = np.mean(scores)\n",
        "    print(\"평균 정확도: {0:.4f}\".format(mean_score)) \n",
        "# exec_kfold 호출\n",
        "exec_kfold(dt_clf , folds=5) "
      ],
      "metadata": {
        "id": "sc-o1Ze_VuFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {'max_depth':[2,3,5,10],\n",
        "             'min_samples_split':[2,3,5], 'min_samples_leaf':[1,5,8]}\n",
        "\n",
        "grid_dclf = GridSearchCV(dt_clf , param_grid=parameters , scoring='accuracy' , cv=5)\n",
        "grid_dclf.fit(X_train , y_train)\n",
        "\n",
        "print('GridSearchCV 최적 하이퍼 파라미터 :',grid_dclf.best_params_)\n",
        "print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dclf.best_score_))\n",
        "best_dclf = grid_dclf.best_estimator_\n",
        "\n",
        "# GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행. \n",
        "dpredictions = best_dclf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test , dpredictions)\n",
        "print('테스트 세트에서의 DecisionTreeClassifier 정확도 : {0:.4f}'.format(accuracy))"
      ],
      "metadata": {
        "id": "QNcA2VyLV2bq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}